---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


Di He (贺笛) is currently a Senior Researcher in Machine Learning Group at Microsoft Research Asia. He obtained his bachelor, master and Ph.D. degrees from Peking University, advised by Liwei Wang.

Di’s main research interests include representation learning (mainly focusing on learning the representation of structured data such as languages and graphs), trust-worthy machine learning, and deep learning optimization. The primary goal of his work is to develop efficient algorithms that can capture accurate and robust features from data through deep neural networks. To achieve this goal, Di focuses on providing a deeper understanding of different neural network architectures for different practical scenarios and their optimization processes. Di has been serving on the PCs and Senior PCs of the top machine learning and artificial intelligence conferences, such as ICML, NIPS, ICLR, AAAI, and IJCAI.

Publications & Preprints
=====
[1] **Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding** (NeurIPS 2021) [[PDF](https://arxiv.org/abs/2106.12566)]  
  Shengjie Luo\*, **Shanda Li**\*, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang, Tie-Yan Liu  
   
   * We propose a new method to accelerate attention calculation for Transformers with relative positional encoding (RPE) on top of the kernelized attention by utilizing Fast Fourier Transform (FFT). 
   * Our method also helps to stabilize training of kernelized attention models.
 
 [2] **Can Vision Transformers Perform Convolution?** (Preprint) [[PDF](https://arxiv.org/abs/2111.01353)]  
  **Shanda Li**, Xiangning Chen, Di He, Cho-Jui Hsieh  
   
   * We prove that a single self-attention layer with image patches as the input can perform any convolution operation as long as the number of heads is sufficient. 
   * We further provide a lower bound on the number of heads for self-attention layers to express convolution. 

Education
=====

**Peking University**, Beijing, China

* B.S. in Computer Science and Technology. 2018 - 2022 (expected).

Service and leadership
=====  

2020.9 - 2021.9, **President**, EECS Students’ Association for Science and Technology, Peking University.
